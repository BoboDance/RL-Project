# A3C - Asynchronous Advantage Actor-Critic 

Source:\
[Asynchronous Methods for Deep Reinforcement Learning
](https://arxiv.org/abs/1602.01783)


## Best Parameter Settings: 

### CartpoleStabShort:  

- lr: 
    - Actor: .0001
    - Critic: .001
- backprop after n steps: 10
- max epsisode steps: 50000
- gamma: .9
- tau: 1
- beta: .01
    
### CartpoleStabLong:  
TODO

- lr: 
    - Actor: .0001
    - Critic: .001
- backprop after n steps: 10
- max epsisode steps: 50000
- gamma: .9
- tau: 1
- beta: .01

### CartpoleSwingShort:  
TODO

- lr: 
    - Actor: .0001
    - Critic: .001
- backprop after n steps: 10
- max epsisode steps: 50000
- gamma: .9
- tau: 1
- beta: .01

### CartpoleSwingLong:  
TODO

- lr: 
    - Actor: .0001
    - Critic: .001
- backprop after n steps: 10
- max epsisode steps: 50000
- gamma: .9
- tau: 1
- beta: .01

### Qube/Furuta Pendulum:  
TODO

- lr: 
    - Actor: .0001
    - Critic: .001
- backprop after n steps: 10
- max epsisode steps: 50000
- gamma: .9
- tau: 1
- beta: .01