\begin{thebibliography}{10}
\providecommand{\url}[1]{{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{DOI~\discretionary{}{}{}#1}\else
  \providecommand{\doi}{DOI~\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem{Abdolmaleki2018}
Abdolmaleki, A., Springenberg, J.T., Tassa, Y., Munos, R., Heess, N.,
  Riedmiller, M.: {Maximum a Posteriori Policy Optimisation}  (2018).
\newblock \urlprefix\url{http://arxiv.org/abs/1806.06920}

\bibitem{Bellemare2013}
Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: {The Arcade Learning
  Environment: An Evaluation Platform for General Agents}.
\newblock Journal of Artificial Intelligence Research \textbf{47}, 253--279
  (2013).
\newblock \doi{10.1613/jair.3912}.
\newblock \urlprefix\url{http://arxiv.org/abs/1207.4708}

\bibitem{Fujimoto2018}
Fujimoto, S., van Hoof, H., Meger, D.: {Addressing Function Approximation Error
  in Actor-Critic Methods}.
\newblock In: International Conference for Machine Learning (2018).
\newblock \urlprefix\url{http://arxiv.org/abs/1802.09477}

\bibitem{Grosse2016}
Grosse, R., Martens, J.: {A Kronecker-factored Approximate Fisher Matrix for
  Convolution Layers}.
\newblock In: Proceedings of the 33rd International Conference on International
  Conference on Machine Learning - Volume 48, ICML'16, pp. 573--582. JMLR.org
  (2016).
\newblock \urlprefix\url{http://dl.acm.org/citation.cfm?id=3045390.3045452}

\bibitem{Gu2016}
Gu, S., Lillicrap, T., Sutskever, I., Levine, S.: {Continuous Deep Q-Learning
  with Model-based Acceleration}.
\newblock In: ICML'16 Proceedings of the 33rd International Conference on
  International Conference onMachine Learning - Volume 48, pp. 2829--2838.
  JMLR.org, New York, NY, USA (2016).
\newblock \urlprefix\url{http://arxiv.org/abs/1603.00748}

\bibitem{Haarnoja2018}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., Levine, S.: {Soft Actor-Critic Algorithms
  and Applications}  (2018).
\newblock \urlprefix\url{http://arxiv.org/abs/1812.05905}

\bibitem{VanHasselt2010}
van Hasselt, H.: {Double Q-learning}.
\newblock In: J.D. Lafferty, C.K.I. Williams, J.~Shawe-Taylor, R.S. Zemel,
  A.~Culotta (eds.) Advances in Neural Information Processing Systems 23, pp.
  2613--2621. Curran Associates, Inc. (2010).
\newblock
  \urlprefix\url{http://papers.nips.cc/paper/3964-double-q-learning.pdf}

\bibitem{VanHasselt2016}
van Hasselt, H., Guez, A., Silver, D.: {Deep Reinforcement Learning with Double
  Q-learning}.
\newblock In: AAAI Conference on Artificial Intelligence, pp. 2094--2100.
  Phoenix, Arizona (2016).
\newblock \urlprefix\url{http://arxiv.org/abs/1509.06461}

\bibitem{Heess2017}
Heess, N., TB, D., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y.,
  Erez, T., Wang, Z., Eslami, S.M.A., Riedmiller, M., Silver, D.: {Emergence of
  Locomotion Behaviours in Rich Environments}  (2017).
\newblock \urlprefix\url{http://arxiv.org/abs/1707.02286}

\bibitem{Hunter2004}
Hunter, D.R., Lange, K.: {A tutorial on MM algorithms}.
\newblock The American Statistician \textbf{58}(1), 30--37 (2004)

\bibitem{Ioffe2015}
Ioffe, S., Szegedy, C.: {Batch Normalization: Accelerating Deep Network
  Training by Reducing Internal Covariate Shift}.
\newblock In: International Conference on Machine Learning (ICML), ICML'15, pp.
  448--456. JMLR.org (2015).
\newblock \urlprefix\url{http://dl.acm.org/citation.cfm?id=3045118.3045167}

\bibitem{Kakade2001}
Kakade, S.: {A Natural Policy Gradient}.
\newblock In: Proceedings of the 14th International Conference on Neural
  Information Processing Systems: Natural and Synthetic, NIPS'01, pp.
  1531--1538. MIT Press, Cambridge, MA, USA (2001).
\newblock \urlprefix\url{http://dl.acm.org/citation.cfm?id=2980539.2980738}

\bibitem{Kulkarni2016}
Kulkarni, T.D., Narasimhan, K., Saeedi, A., Tenenbaum, J.: {Hierarchical Deep
  Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic
  Motivation}.
\newblock In: D.D. Lee, M.~Sugiyama, U.V. Luxburg, I.~Guyon, R.~Garnett (eds.)
  Advances in Neural Information Processing Systems 29, pp. 3675--3683. Curran
  Associates, Inc. (2016).
\newblock
  \urlprefix\url{http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf}

\bibitem{Lillicrap2016}
Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., Wierstra, D.: {Continuous control with deep reinforcement
  learning}.
\newblock In: International Conference on Learning Representations (ICLR) 2016.
  London, UK (2016).
\newblock \urlprefix\url{http://arxiv.org/abs/1509.02971}

\bibitem{Lin1992}
Lin, L.J.: {Self-improving reactive agents based on reinforcement learning,
  planning and teaching}.
\newblock Machine Learning \textbf{8}(3), 293--321 (1992).
\newblock \doi{10.1007/BF00992699}.
\newblock \urlprefix\url{https://doi.org/10.1007/BF00992699}

\bibitem{Martens2015}
Martens, J., Grosse, R.: {Optimizing Neural Networks with Kronecker-factored
  Approximate Curvature}  (2015).
\newblock \urlprefix\url{http://arxiv.org/abs/1503.05671}

\bibitem{Mnih2016}
Mnih, V., Badia, A.P.A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T.,
  Silver, D., Kavukcuoglu, K.: {Asynchronous Methods for Deep Reinforcement
  Learning}.
\newblock In: M.F. Balcan, K.Q. Weinberger (eds.) Proceedings of The 33rd
  International Conference on Machine Learning, \emph{Proceedings of Machine
  Learning Research}, vol.~48, pp. 1928--1937. PMLR, New York, New York, USA
  (2016).
\newblock \urlprefix\url{http://proceedings.mlr.press/v48/mniha16.html
  http://arxiv.org/abs/1602.01783}

\bibitem{Mnih2013}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., Riedmiller, M.: {Playing Atari with Deep Reinforcement Learning}.
\newblock NIPS Deep Learning Workshop 2013  (2013).
\newblock \urlprefix\url{http://arxiv.org/abs/1312.5602}

\bibitem{Mnih2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
  Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S.,
  Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
  Legg, S., Hassabis, D.: {Human-level control through deep reinforcement
  learning}.
\newblock Nature \textbf{518}(7540), 529--533 (2015).
\newblock \doi{10.1038/nature14236}.
\newblock \urlprefix\url{http://www.nature.com/articles/nature14236}

\bibitem{Riedmiller2005}
Riedmiller, M.: {Neural Fitted Q Iteration - First Experiences with a Data
  Efficient Neural Reinforcement Learning Method}.
\newblock In: J.~Gama, R.~Camacho, P.B. Brazdil, A.M. Jorge, L.~Torgo (eds.)
  Machine Learning: ECML 2005, pp. 317--328. Springer Berlin Heidelberg,
  Berlin, Heidelberg (2005)

\bibitem{Riedmiller1993}
Riedmiller, M., Braun, H.: {A Direct Adaptive Method for Faster Backpropagation
  Learning: The RPROP Algorithm}.
\newblock In: IEEE International Conference on Neural Networks (IJCNN), pp.
  586--591 (1993)

\bibitem{Schaul2015}
Schaul, T., Quan, J., Antonoglou, I., Silver, D.: {Prioritized Experience
  Replay}.
\newblock In: Interantional Conference for Learning Representations (ICLR)
  (2015).
\newblock \urlprefix\url{http://arxiv.org/abs/1511.05952}

\bibitem{Schulman2015}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: {Trust Region
  Policy Optimization}.
\newblock In: F.~Bach, D.~Blei (eds.) Proc. 32nd Int. Conf. Mach. Learn.,
  \emph{Proceedings of Machine Learning Research}, vol.~37, pp. 1889--1897.
  PMLR, Lille, France (2015).
\newblock \urlprefix\url{http://proceedings.mlr.press/v37/schulman15.html}

\bibitem{Schulman2017}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: {Proximal
  Policy Optimization Algorithms}  (2017).
\newblock \urlprefix\url{http://arxiv.org/abs/1707.06347}

\bibitem{VanSeijen2009}
van Seijen, H., van Hasselt, H., Whiteson, S., Wiering, M.: {A theoretical and
  empirical analysis of Expected Sarsa}.
\newblock In: 2009 IEEE Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning, pp. 177--184 (2009).
\newblock \doi{10.1109/ADPRL.2009.4927542}

\bibitem{Silver2014}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M.:
  {Deterministic Policy Gradient Algorithms}.
\newblock In: International Conference on Machine Learning (ICML), ICML'14, pp.
  I--387----I--395. JMLR.org, Beijing, China (2014).
\newblock \urlprefix\url{http://dl.acm.org/citation.cfm?id=3044805.3044850}

\bibitem{Sutton1990}
Sutton, R.S.: {Integrated architectures for learning, planning, reacting based
  on approxmiate dynmaic programming}.
\newblock In: International Conference for Machine Learning (ICML)2, pp.
  216--224 (1990)

\bibitem{Sutton2018}
Sutton, R.S., Barto, A.G.: {Reinforcement learning: An introduction}, second
  edi edn.
\newblock MIT Press, Cambridge, MA (2018).
\newblock \urlprefix\url{http://incompleteideas.net/book/the-book.html}

\bibitem{Tesauro1994}
Tesauro, G.: {TD-Gammon, a Self-Teaching Backgammon Program, Achieves
  Master-Level Play}.
\newblock Applications of Neural Networks \textbf{6}(2), 215--219 (1994).
\newblock \doi{10.1162/neco.1994.6.2.215}.
\newblock \urlprefix\url{https://doi.org/10.1162/neco.1994.6.2.215}

\bibitem{Todorov2005}
Todorov, E., Li, W.: {A generalized iterative LQG method for locally-optimal
  feedback control of constrained nonlinear stochastic systems}.
\newblock In: American Control Conference, pp. 300--306 vol. 1 (2005).
\newblock \doi{10.1109/ACC.2005.1469949}

\bibitem{Wang2016}
Wang, Z., Schaul, T., Hessel, M., {Van Hasselt}, H., Lanctot, M., {De Freitas},
  N.: {Dueling Network Architectures for Deep Reinforcement Learning}.
\newblock In: Proceedings of the 33rd International Conference on International
  Conference on Machine Learning - Volume 48, ICML'16, pp. 1995--2003. JMLR.org
  (2016).
\newblock \urlprefix\url{http://dl.acm.org/citation.cfm?id=3045390.3045601}

\bibitem{Watkins1989}
Watkins, C.J.C.H.: {Learning from delayed rewards}.
\newblock Ph.D. thesis, King's College, Cambridge (1989)

\bibitem{Williams1992}
Williams, R.J.: {Simple statistical gradient-following algorithms for
  connectionist reinforcement learning}.
\newblock Machine Learning \textbf{8}(3), 229--256 (1992).
\newblock \doi{10.1007/BF00992696}.
\newblock \urlprefix\url{https://doi.org/10.1007/BF00992696}

\bibitem{Wu2017}
Wu, Y., Mansimov, E., Grosse, R.B., Liao, S., Ba, J.: {Scalable trust-region
  method for deep reinforcement learning using Kronecker-factored
  approximation}.
\newblock In: I.~Guyon, U.V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, R.~Garnett (eds.) Advances in Neural Information Processing
  Systems 30, pp. 5279--5288. Curran Associates, Inc. (2017).
\newblock
  \urlprefix\url{http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf}

\end{thebibliography}
