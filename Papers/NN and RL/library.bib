Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Sutton1990,
author = {Sutton, Richard S},
booktitle = {International Conference for Machine Learning (ICML)2},
pages = {216--224},
title = {{Integrated architectures for learning, planning, reacting based on approxmiate dynmaic programming}},
year = {1990}
}
@article{Zambaldi2018a,
abstract = {We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.},
archivePrefix = {arXiv},
arxivId = {1806.01830},
author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter},
eprint = {1806.01830},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zambaldi et al. - 2018 - Relational Deep Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Relational Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01830},
year = {2018}
}
@inproceedings{Gu2016,
abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
booktitle = {ICML'16 Proceedings of the 33rd International Conference on International Conference onMachine Learning - Volume 48},
eprint = {1603.00748},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2016 - Continuous Deep Q-Learning with Model-based Acceleration.pdf:pdf},
month = {mar},
pages = {2829--2838},
publisher = {JMLR.org},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}},
url = {http://arxiv.org/abs/1603.00748},
year = {2016}
}
@inproceedings{Schaul2015,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
booktitle = {Interantional Conference for Learning Representations (ICLR)},
eprint = {1511.05952},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2015 - Prioritized Experience Replay.pdf:pdf},
month = {nov},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2015}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Yang, Cheung - 2015 - A mechanised 3D scanning method for item-level radio frequency identification of palletised products.pdf:pdf},
journal = {NIPS Deep Learning Workshop 2013},
month = {dec},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@inproceedings{7989383,
abstract = {Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without extensive manual engineering. However, robotic skill learning must typically make trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human demonstrations, instrumentation of the training environment, or extremely long training times. We propose a new reinforcement learning algorithm that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. We build on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle random initial states, allowing it to be used even when deterministic resets are impossible. We compare our method to existing policy search algorithms in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and can learn policies directly from image pixels. We also present real-world robot results that show that our method can learn manipulation policies with visual features and random initial states.},
annote = {basic for MAP},
author = {Montgomery, W and Ajay, A and Finn, C and Abbeel, P and Levine, S},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989383},
file = {:home/fabian/Desktop/Reset-free guided policy search$\backslash$: Efficient deep reinforcement learning with stochastic initial states.pdf:pdf},
keywords = {GPS algorithm,Global Positioning System,Learning (artificial intelligence),Neural networks,Optimization,Robots,Supervised learning,Training,autonomous learning,deep reinforcement learning,general-purpose neural network policies,high-dimensional neural network policies,image pixels,learning (artificial intelligence),neurocontrollers,reset-free guided policy search,robotic skill learning,robots,stochastic initial states},
month = {may},
pages = {3373--3380},
title = {{Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states}},
year = {2017}
}
@article{Crites1996,
abstract = {This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimiz...},
author = {Crites, Robert and Crites, Robert and Barto, Andrew},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crites, Crites, Barto - 1996 - Improving Elevator Performance Using Reinforcement Learning.pdf:pdf},
journal = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8},
pages = {1017----1023},
title = {{Improving Elevator Performance Using Reinforcement Learning}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.5519},
volume = {8},
year = {1996}
}
@unpublished{Li2018,
abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
archivePrefix = {arXiv},
arxivId = {1810.06339},
author = {Li, Yuxi},
eprint = {1810.06339},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2018 - Deep Reinforcement Learning.pdf:pdf},
institution = {Morgan {\&} Claypool: Synthesis Lectures in Artificial Intelligence and Machine Learning},
month = {oct},
title = {{Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1810.06339},
year = {2018}
}
@inproceedings{7989466,
abstract = {Learning from demonstrations has been shown to be a successful method for non-experts to teach manipulation tasks to robots. These methods typically build generative models from demonstrations and then use regression to reproduce skills. However, this approach has limitations to capture hard geometric constraints imposed by the task. On the other hand, while sampling and optimization-based motion planners exist that reason about geometric constraints, these are typically carefully hand-crafted by an expert. To address this technical gap, we contribute with C-LEARN, a method that learns multi-step manipulation tasks from demonstrations as a sequence of keyframes and a set of geometric constraints. The system builds a knowledge base for reaching and grasping objects, which is then leveraged to learn multi-step tasks from a single demonstration. C-LEARN supports multi-step tasks with multiple end effectors; reasons about SE(3) volumetric and CAD constraints, such as the need for two axes to be parallel; and offers a principled way to transfer skills between robots with different kinematics. We embed the execution of the learned tasks within a shared autonomy framework, and evaluate our approach by analyzing the success rate when performing physical tasks with a dual-arm Optimas robot, comparing the contribution of different constraints models, and demonstrating the ability of C-LEARN to transfer learned tasks by performing them with a legged dual-arm Atlas robot in simulation.},
author = {P{\'{e}}rez-D'Arpino, C and Shah, J A},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989466},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez-D'Arpino, Shah - 2017 - C-LEARN Learning geometric constraints from demonstrations for multi-step manipulation in shared autonomy.pdf:pdf},
keywords = {C-LEARN,End effectors,Grasping,Hidden Markov models,Knowledge based systems,Planning,Solid modeling,control engineering computing,dual-arm Optimas robot,end effectors,knowledge base,knowledge based systems,learning (artificial intelligence),learning geometric constraints,legged dual-arm Atlas robot,legged locomotion,manipulator kinematics,multistep manipulation,multistep tasks,optimization-based motion planners,path planning,regression,regression analysis,robot kinematics,sampling methods,sampling-based motion planners,shared autonomy},
pages = {4058--4065},
title = {{C-LEARN: Learning geometric constraints from demonstrations for multi-step manipulation in shared autonomy}},
year = {2017}
}
@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1002/acn3.501},
eprint = {1712.01815},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:pdf},
isbn = {3013372370},
issn = {23289503},
month = {dec},
pages = {1--19},
pmid = {29376095},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview(2).pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}
@incollection{NIPS2017_7217,
author = {Lowe, Ryan and WU, Y I and Tamar, Aviv and Harb, Jean and {Pieter Abbeel}, OpenAI and Mordatch, Igor},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:home/fabian/Desktop/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf:pdf},
pages = {6379--6390},
publisher = {Curran Associates, Inc.},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf},
year = {2017}
}
@article{Tesauro1994,
abstract = {TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD($\lambda$) reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a “raw” description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.},
address = {Boston, MA},
author = {Tesauro, Gerald},
doi = {10.1162/neco.1994.6.2.215},
editor = {Murray, Alan F},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Networks, 1995 - 1993 - Td-gammon A self-teaching backgammon program.pdf:pdf},
isbn = {978-1-4757-2379-3},
journal = {Applications of Neural Networks},
number = {2},
pages = {215--219},
publisher = {Springer US},
title = {{TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play}},
url = {https://doi.org/10.1162/neco.1994.6.2.215},
volume = {6},
year = {1994}
}
@incollection{Chua2018,
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models.pdf:pdf},
pages = {4759--4770},
publisher = {Curran Associates, Inc.},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf},
year = {2018}
}
@inproceedings{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
address = {London, UK},
annote = {From Duplicate 1 (Continuous control with deep reinforcement learning - Lillicrap, Timothy P.; Hunt, Jonathan J.; Pritzel, Alexander; Heess, Nicolas; Erez, Tom; Tassa, Yuval; Silver, David; Wierstra, Daan)

Deep Deterministic Policy Gradients. Lillicrap et al. (2016) introduced a policy gradient algorithm, DDPG, that works with deterministic policies. This is an actor-critic method that uses neural networks for both the Q function and the policy. After each time step, the new experience sample is stored in an ‘experience replay' buffer, a randomly sampled batch from this buffer is then used to calculate the policy gradient. This technique diminishes the problem that successive samples tend to be highly correlated. We again used the RLlab implementation (Duan et al., 2016) with fixed episode length.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {International Conference on Learning Representations (ICLR) 2016},
eprint = {1509.02971},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning.pdf:pdf},
month = {sep},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2016}
}
@inproceedings{Silver2014,
address = {Beijing, China},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/fabian/Desktop/Deterministic Policy Gradient Algorithms.pdf:pdf},
pages = {I--387----I--395},
publisher = {JMLR.org},
series = {ICML'14},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://dl.acm.org/citation.cfm?id=3044805.3044850},
year = {2014}
}
@article{Wierstra2014,
author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
file = {:home/fabian/Desktop/Natural Evolution Strategies.pdf:pdf},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {black-box optimization,evolution strategies,natural gradient,sampling,stochastic search},
month = {jan},
number = {1},
pages = {949--980},
publisher = {JMLR.org},
title = {{Natural Evolution Strategies}},
url = {http://dl.acm.org/citation.cfm?id=2627435.2638566},
volume = {15},
year = {2014}
}
@inproceedings{Zhu2017,
abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment. The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.},
address = {Singapore, Singapore},
archivePrefix = {arXiv},
arxivId = {1609.05143},
author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1609.05143},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Target-driven visual navigation in indoor scenes using deep reinforcement learning.pdf:pdf},
isbn = {978-1-5090-4633-1},
month = {may},
pages = {3357--3364},
publisher = {IEEE},
title = {{Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning}},
url = {http://ieeexplore.ieee.org/document/7989381/},
year = {2017}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@incollection{NIPS2017_7112,
author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:home/fabian/Desktop/Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
pages = {5279--5288},
publisher = {Curran Associates, Inc.},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf},
year = {2017}
}
@inproceedings{Hagan1999,
abstract = {Provides a quick overview of neural networks and explains how they can be used in control systems. We introduce the multilayer perceptron neural network and describe how it can be used for function approximation. The backpropagation algorithm (including its variations) is the principal procedure for training multilayer perceptrons; it is briefly described here. Care must be taken, when training perceptron networks, to ensure that they do not overfit the training data and then fail to generalize well in new situations. Several techniques for improving generalization are discussed. The article also presents several control architectures, such as model reference adaptive control, model predictive control, and internal model control, in which multilayer perceptron neural networks can be used as basic building blocks.},
address = {San Diego, CA, USA},
author = {Hagan, M.T. and Demuth, H.B.},
booktitle = {Proceedings of the 1999 American Control Conference (Cat. No. 99CH36251)},
doi = {10.1109/ACC.1999.786109},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hagan, Demuth - 1999 - Neural networks for control.pdf:pdf},
isbn = {0-7803-4990-3},
pages = {1642--1656 vol.3},
publisher = {IEEE},
title = {{Neural networks for control}},
url = {http://ieeexplore.ieee.org/document/786109/},
year = {1999}
}
@inproceedings{Lange2012,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne},
booktitle = {The 2012 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2012.6252823},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller, Voigtlander - 2012 - Autonomous reinforcement learning on raw visual input data in a real world application.pdf:pdf},
isbn = {978-1-4673-1490-9},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
url = {http://ieeexplore.ieee.org/document/6252823/},
year = {2012}
}
@inproceedings{Caicedo2015,
abstract = {We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
address = {Washington, DC, USA},
archivePrefix = {arXiv},
arxivId = {1511.06015},
author = {Caicedo, Juan C. and Lazebnik, Svetlana},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.286},
eprint = {1511.06015},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caicedo, Lazebnik - 2015 - Active Object Localization with Deep Reinforcement Learning.pdf:pdf},
month = {nov},
pages = {2488--2496},
publisher = {IEEE},
title = {{Active Object Localization with Deep Reinforcement Learning}},
url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Caicedo{\_}Active{\_}Object{\_}Localization{\_}ICCV{\_}2015{\_}paper.pdf http://arxiv.org/abs/1511.06015},
year = {2015}
}
@article{Silver2016,
abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {jan},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/articles/nature16961},
volume = {529},
year = {2016}
}
@article{Meeden1993,
author = {Meeden, Lisa and Meeden, Lisa and Mcgraw, Gary and Blank, Douglas},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meeden et al. - 1993 - Emergent Control and Planning in an Autonomous Vehicle.pdf:pdf},
journal = {Proceedings of the Fifteenth Annual Meeting of the Cognitive ScienceSociety},
pages = {735--740},
title = {{Emergent Control and Planning in an Autonomous Vehicle}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.3932},
year = {1993}
}
@article{Deisenroth:2013:SPS:2688186.2688187,
address = {Hanover, MA, USA},
author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
doi = {10.1561/2300000021},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deisenroth, Neumann, Peters - 2013 - A Survey on Policy Search for Robotics.pdf:pdf},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
month = {aug},
number = {1-2},
pages = {1--142},
publisher = {Now Publishers Inc.},
title = {{A Survey on Policy Search for Robotics}},
url = {http://dx.doi.org/10.1561/2300000021},
volume = {2},
year = {2013}
}
@incollection{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting $\backslash$textit{\{}Deep Recurrent Q-Network{\}} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
booktitle = {AAAI Fall Symposia},
eprint = {1507.06527},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hausknecht, Stone - 2015 - Deep Recurrent Q-Learning for Partially Observable MDPs.pdf:pdf},
month = {jul},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
year = {2015}
}
@inproceedings{Schmidhuber1991,
abstract = {{A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.{\textless}{\textless}ETX{\textgreater}{\textgreater}{\}}, keywords={\{}adaptive control;adaptive systems;learning systems;neural nets;adaptive control;adaptive systems;learning systems;model-building control systems;four-network system;Watkins' Q-learning algorithm;temporal derivative;adaptive assumed reliability;future predictions;artificial nondeterministic environment;Control system synthesis;Predictive models;Adaptive control;Programmable control;Learning;Computer science;Manipulator dynamics;Computer architecture;Computer networks;Error correction}},
author = {Schmidhuber, J.},
booktitle = {1991 IEEE International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.1991.170605},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 1991 - Curious model-building control systems.pdf:pdf},
isbn = {0-7803-0227-3},
pages = {1458--1463},
publisher = {IEEE},
title = {{Curious model-building control systems}},
url = {http://ieeexplore.ieee.org/document/170605/},
year = {1991}
}
@inproceedings{Nagabandi2018,
author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2018.8463189},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagabandi et al. - 2018 - Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.pdf:pdf},
isbn = {978-1-5386-3081-5},
month = {may},
pages = {7559--7566},
publisher = {IEEE},
title = {{Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning}},
url = {https://ieeexplore.ieee.org/document/8463189/},
year = {2018}
}
@incollection{NIPS1994_1018,
author = {Boyan, Justin A and Moore, Andrew W},
booktitle = {Advances in Neural Information Processing Systems 7},
editor = {Tesauro, G and Touretzky, D S and Leen, T K},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan, Moore - 1995 - Generalization in Reinforcement Learning Safely Approximating the Value Function.pdf:pdf},
pages = {369--376},
publisher = {MIT Press},
title = {{Generalization in Reinforcement Learning: Safely Approximating the Value Function}},
url = {http://papers.nips.cc/paper/1018-generalization-in-reinforcement-learning-safely-approximating-the-value-function.pdf},
year = {1995}
}
@book{Sutton2018,
address = {Cambridge, MA},
author = {Sutton, Richard S and Barto, Andrew G},
edition = {Second Edi},
publisher = {MIT Press},
title = {{Reinforcement learning: An introduction}},
url = {http://incompleteideas.net/book/the-book.html},
year = {2018}
}
@inproceedings{Todorov2005,
abstract = {We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu//spl sim/todorov.},
author = {Todorov, E and Li, Weiwei},
booktitle = {American Control Conference},
doi = {10.1109/ACC.2005.1469949},
issn = {0743-1619},
keywords = {Control systems,Convergence,Costs,Feedback control,Iterative methods,Levenberg-Marquardt method,Linear feedback control systems,Mathematical model,Matlab,Nonlinear control systems,Stochastic processes,Stochastic systems,actuators,biomechanical control problem,biomechanics,constrained nonlinear stochastic systems,constraint theory,control constraints,convergence,deterministic unconstrained problems,feedback,generalized iterative LQG method,global convergence,human arm,iterative linear-quadratic-Gaussian method,iterative methods,limited-torque inverted pendulum,linear quadratic Gaussian control,locally-optimal feedback control,muscle,muscle actuators,nonlinear systems,optimal control,optimal cost-to-go function,pendulums,quadratic approximation,quadratic cost,quadratic programming,state dimensions,stochastic model,stochastic systems},
month = {jun},
pages = {300--306 vol. 1},
title = {{A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems}},
year = {2005}
}
@inproceedings{VanHasselt2016,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
address = {Phoenix, Arizona},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {1509.06461},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Hasselt, Guez, Silver - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:pdf},
month = {sep},
pages = {2094--2100},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461},
year = {2016}
}
@article{Kaelbling1996,
abstract = {{\textless}p{\textgreater}This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.{\textless}/p{\textgreater}},
author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
doi = {10.1613/jair.301},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaelbling, Littman, Moore - 1996 - Reinforcement Learning A Survey.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
pages = {237--285},
title = {{Reinforcement Learning: A Survey}},
url = {https://jair.org/index.php/jair/article/view/10166},
volume = {4},
year = {1996}
}
@inproceedings{Peng2017,
abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1710.06537},
author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
booktitle = {Conference on Neural Information Processing Systems (NIPS)},
doi = {10.1109/ICRA.2018.8460528},
eprint = {1710.06537},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2017 - Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.pdf:pdf},
month = {oct},
title = {{Sim-to-Real Transfer of Robotic Control with Dynamics Randomization}},
url = {http://arxiv.org/abs/1710.06537 http://dx.doi.org/10.1109/ICRA.2018.8460528},
year = {2017}
}
@article{Lin1992,
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
annote = {replay memory basic paper --{\textgreater} off policy},
author = {Lin, Long-Ji},
doi = {10.1007/BF00992699},
issn = {1573-0565},
journal = {Machine Learning},
month = {may},
number = {3},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
url = {https://doi.org/10.1007/BF00992699},
volume = {8},
year = {1992}
}
@inproceedings{10.1007/11552246_35,
abstract = {Helicopters have highly stochastic, nonlinear, dynamics, and autonomous helicopter flight is widely regarded to be a challenging control problem. As helicopters are highly unstable at low speeds, it is particularly difficult to design controllers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopter's dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform.},
address = {Berlin, Heidelberg},
author = {Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
booktitle = {Experimental Robotics IX},
editor = {Ang, Marcelo H and Khatib, Oussama},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng et al. - 2006 - Autonomous Inverted Helicopter Flight via Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-33014-1},
pages = {363--372},
publisher = {Springer Berlin Heidelberg},
title = {{Autonomous Inverted Helicopter Flight via Reinforcement Learning}},
year = {2006}
}
@inproceedings{Lange2012,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne},
booktitle = {2012 Int. Jt. Conf. Neural Networks},
doi = {10.1109/IJCNN.2012.6252823},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller, Voigtlander - 2012 - Autonomous reinforcement learning on raw visual input data in a real world application.pdf:pdf},
isbn = {978-1-4673-1490-9},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
url = {http://ieeexplore.ieee.org/document/6252823/},
year = {2012}
}
@inproceedings{Riedmiller1993,
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {IEEE International Conference on Neural Networks (IJCNN)},
pages = {586--591},
title = {{A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm}},
year = {1993}
}
@book{Miikkulainen2002,
address = {San Francisco},
author = {Miikkulainen, Risto and Stanley, Kenneth O.},
booktitle = {Genetic and Evolutionary Computation Conference (GECCO-2002)},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miikkulainen, Stanley - 2002 - Efficient Reinforcement Learning through Evolving Neural Network Topologies.pdf:pdf},
isbn = {1558608788},
pages = {9},
publisher = {Morgan Kaufmann},
title = {{Efficient Reinforcement Learning through Evolving Neural Network Topologies}},
url = {http://nn.cs.utexas.edu/?stanley:gecco02b},
year = {2002}
}
@inproceedings{Cuccu2011,
abstract = {Neuroevolution, the artificial evolution of neural networks, has shown great promise on continuous reinforcement learning tasks that require memory. However, it is not yet directly applicable to realistic embedded agents using high-dimensional (e.g. raw video images) inputs, requiring very large networks. In this paper, neuroevolution is combined with an unsupervised sensory pre-processor or compressor that is trained on images generated from the environment by the population of evolving recurrent neural network controllers. The compressor not only reduces the input cardinality of the controllers, but also biases the search toward novel controllers by rewarding those controllers that discover images that it reconstructs poorly. The method is successfully demonstrated on a vision-based version of the well-known mountain car benchmark, where controllers receive only single high-dimensional visual images of the environment, from a third-person perspective, instead of the standard two-dimensional state vector which includes information about velocity.},
annote = {Applies NFQ (Riedmiller, 2005)},
author = {Cuccu, Giuseppe and Luciw, Matthew and Schmidhuber, Jurgen and Gomez, Faustino},
booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
doi = {10.1109/DEVLRN.2011.6037324},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cuccu et al. - 2011 - Intrinsically motivated neuroevolution for vision-based reinforcement learning.pdf:pdf},
isbn = {978-1-61284-989-8},
month = {aug},
pages = {1--7},
publisher = {IEEE},
title = {{Intrinsically motivated neuroevolution for vision-based reinforcement learning}},
url = {http://ieeexplore.ieee.org/document/6037324/},
year = {2011}
}
@article{Bellemare2013,
abstract = {{\textless}p{\textgreater}In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {253--279},
title = {{The Arcade Learning Environment: An Evaluation Platform for General Agents}},
url = {http://arxiv.org/abs/1207.4708},
volume = {47},
year = {2013}
}
@book{Bellman:DynamicProgramming,
abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.},
author = {Bellman, Richard},
isbn = {9780486428093},
keywords = {book dynamic programming},
publisher = {Dover Publications},
title = {{Dynamic Programming}},
year = {1957}
}
@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {448--456},
publisher = {JMLR.org},
series = {ICML'15},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
year = {2015}
}
@incollection{Kober2012,
abstract = {As most action generation problems of autonomous robots can be phrased in terms of sequential decision problems, robotics offers a tremendously important and interesting application platform for reinforcement learning. Similarly, the real-world challenges of this domain pose a major real-world check for reinforcement learning. Hence, the interplay between both disciplines can be seen as promising as the one between physics and mathematics. Nevertheless, only a fraction of the scientists working on reinforcement learning are sufficiently tied to robotics to oversee most problems encountered in this context. Thus, we will bring the most important challenges faced by robot reinforcement learning to their attention. To achieve this goal, we will attempt to survey most work that has successfully applied reinforcement learning to behavior generation for real robots. We discuss how the presented successful approaches have been made tractable despite the complexity of the domain and will study how representations or the inclusion of prior knowledge can make a significant difference. As a result, a particular focus of our chapter lies on the choice between model-based and model-free as well as between value function-based and policy search methods. As a result, we obtain a fairly complete survey of robot reinforcement learning which should allow a general reinforcement learning researcher to understand this domain.},
address = {Berlin, Heidelberg},
author = {Kober, Jens and Peters, Jan},
booktitle = {Reinforcement Learning: State-of-the-Art},
doi = {10.1007/978-3-642-27645-3_18},
editor = {Wiering, Marco and van Otterlo, Martijn},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kober, Peters - 2012 - Reinforcement Learning in Robotics A Survey.pdf:pdf},
isbn = {978-3-642-27645-3},
pages = {579--610},
publisher = {Springer Berlin Heidelberg},
title = {{Reinforcement Learning in Robotics: A Survey}},
url = {https://doi.org/10.1007/978-3-642-27645-3{\_}18},
year = {2012}
}
@inproceedings{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
eprint = {1602.01783},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning(2).pdf:pdf},
month = {feb},
pages = {1928--1937},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://proceedings.mlr.press/v48/mniha16.html http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@inproceedings{10.1007/978-3-319-09165-5_6,
abstract = {In recent years the Monte Carlo tree search revolution has spread from computer Go to many areas, including computer Hex. MCTS-based Hex players now outperform traditional knowledge-based alpha-beta search players, and the reigning Computer Olympiad Hex gold medallist is the MCTS player MoHex. In this paper we show how to strengthen MoHex, and observe that---as in computer Go---using learned patterns in priors and replacing a hand-crafted simulation policy by a softmax policy that uses learned patterns significantly increases playing strength. The result is MoHex 2.0, about 250 Elo points stronger than MoHex on the 11{\$}{\$}$\backslash$times {\$}{\$}{\{}$\backslash$texttimes{\}}11 board, and 300 Elo points stronger on the 13{\$}{\$}$\backslash$times {\$}{\$}{\{}$\backslash$texttimes{\}}13 board.},
address = {Cham},
author = {Huang, Shih-Chieh and Arneson, Broderick and Hayward, Ryan B and M{\"{u}}ller, Martin and Pawlewicz, Jakub},
booktitle = {Computers and Games},
editor = {van den Herik, H Jaap and Iida, Hiroyuki and Plaat, Aske},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2014 - MoHex 2.0 A Pattern-Based MCTS Hex Player.pdf:pdf},
isbn = {978-3-319-09165-5},
pages = {60--71},
publisher = {Springer International Publishing},
title = {{MoHex 2.0: A Pattern-Based MCTS Hex Player}},
year = {2014}
}
@inproceedings{Wang2016,
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Freitas}, Nando},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
file = {:home/fabian/Desktop/Dueling Network Architectures for Deep Reinforcement Learning.pdf:pdf},
pages = {1995--2003},
publisher = {JMLR.org},
series = {ICML'16},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?id=3045390.3045601},
year = {2016}
}
@inproceedings{Schulman2015,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
address = {Lille, France},
annote = {Trust Region Policy Optimization. This method, introduced by Schulman et al. (2015), uses a bound on the expected Kullback-Leibler divergence between successive policies, inspired by a theoretical policy iteration algorithm that guarantees non-decreasing expected returns. The method, TRPO, requires a parametric policy to be defined, but has been shown to work well with deep neural-network policies that can be used in many different tasks thanks to their flexibility. We use the reference implementation provided in the RLlab framework (Duan et al., 2016). This TRPO implementation requires a fixed time horizon, so we evaluate this model on a modified version of the tasks where the episodes are of fixed length, equal to the expected episode length used for other methods.},
author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
booktitle = {Proc. 32nd Int. Conf. Mach. Learn.},
editor = {Bach, Francis and Blei, David},
file = {:home/fabian/Desktop/Trust Region Policy Optimization.pdf:pdf},
pages = {1889--1897},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Trust Region Policy Optimization}},
url = {http://proceedings.mlr.press/v37/schulman15.html},
volume = {37},
year = {2015}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher John Cornish Hellaby},
school = {King's College, Cambridge},
title = {{Learning from delayed rewards}},
year = {1989}
}
@inproceedings{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
address = {Berlin, Heidelberg},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedmiller - 2005 - Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
isbn = {978-3-540-31692-3},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
year = {2005}
}
@article{Bakker2003,
abstract = {It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: (1) reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; (2) online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology's feasibility.},
author = {Bakker, Bram and Zhumatiy, V and Gruener, G and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/IROS.2003.1250667},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakker et al. - 2003 - A robot that reinforcement-learns to identify and memorize important previous observations.pdf:pdf},
isbn = {0-7803-7860-1},
journal = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
pages = {430--435},
pmid = {11465086},
title = {{A robot that reinforcement-learns to identify and memorize important previous observations}},
volume = {1},
year = {2003}
}
@article{article,
author = {Fujimoto, Scott and van Hoof, Herke and Meger, Dave},
file = {:home/fabian/Desktop/Addressing Function Approximation Error in Actor-Critic Methods.pdf:pdf},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
year = {2018}
}
@inproceedings{Gu2017,
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989385},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2017 - Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates.pdf:pdf},
isbn = {978-1-5090-4633-1},
month = {may},
pages = {3389--3396},
publisher = {IEEE},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}},
url = {http://ieeexplore.ieee.org/document/7989385/},
year = {2017}
}
@article{Lange2010,
abstract = {This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Lange, Sascha and Riedmiller, Martin},
doi = {10.1109/IJCNN.2010.5596468},
eprint = {1507.04296},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller - 2010 - Deep auto-encoder neural networks in reinforcement learning.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
pages = {1--9},
pmid = {25719670},
title = {{Deep auto-encoder neural networks in reinforcement learning}},
volume = {2600},
year = {2010}
}
@inproceedings{7989379,
abstract = {Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can also acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when learning to control safety-critical systems. We propose PLATO, a continuous, reset-free reinforcement learning algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision, hence never in need of running a partially trained and potentially unsafe policy. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy in order to generate training samples at states that are likely to be visited by the learned policy. PLATO also maintains the MPC cost as an objective to avoid highly undesirable actions that would result from strictly following the learned policy before it has been fully trained. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy.},
author = {Kahn, G and Zhang, T and Levine, S and Abbeel, P},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989379},
file = {:home/fabian/Desktop/PLATO$\backslash$: Policy Learning using Adaptive Trajectory Optimization.pdf:pdf},
keywords = {Learning (artificial intelligence),Neural networks,PLATO,Robots,Robustness,Supervised learning,Training,Trajectory optimization,adaptive MPC,adaptive control,adaptive training method,aerospace robotics,autonomous systems,complex control policies,continuous reset-free reinforcement learning algor,continuous systems,large-scale systems,learning systems,model-predictive control,policy learning using adaptive trajectory optimiza,policy search,predictive control,robots,simulated aerial vehicle tasks,supervised learning,trajectory optimisation (aerospace)},
month = {may},
pages = {3342--3349},
title = {{PLATO: Policy learning using adaptive trajectory optimization}},
year = {2017}
}
@inproceedings{10.1007/978-3-030-01054-6_1,
abstract = {ViZDoom is a robust, first-person shooter reinforcement learning environment, characterized by a significant degree of latent state information. In this paper, double-Q learning and prioritized experience replay methods are tested under a certain ViZDoom combat scenario using a competitive deep recurrent Q-network (DRQN) architecture. In addition, an ensembling technique known as snapshot ensembling is employed using a specific annealed learning rate to observe differences in ensembling efficacy under these two methods. Annealed learning rates are important in general to the training of deep neural network models, as they shake up the status-quo and counter a model's tending towards local optima. While both variants show performance exceeding those of built-in AI agents of the game, the known stabilizing effects of double-Q learning are illustrated, and priority experience replay is again validated in its usefulness by showing immediate results early on in agent development, with the caveat that value overestimation is accelerated in this case. In addition, some unique behaviors are observed to develop for priority experience replay (PER) and double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves a valuable method for improving performance of the ViZDoom Marine.},
address = {Cham},
author = {Schulze, Christopher and Schulze, Marcus},
booktitle = {Intelligent Systems and Applications},
editor = {Arai, Kohei and Kapoor, Supriya and Bhatia, Rahul},
file = {:home/fabian/Desktop/ViZDoom$\backslash$: DRQN with Prioritized Experience Replay, Double-Q Learning, {\&} Snapshot Ensembling.pdf:pdf},
isbn = {978-3-030-01054-6},
pages = {1--17},
publisher = {Springer International Publishing},
title = {{ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning and Snapshot Ensembling}},
year = {2019}
}
@incollection{Heess2015,
author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
editor = {Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and Garnett, R},
file = {:home/fabian/Desktop/Learning Continuous Control Policies by Stochastic Value Gradients.pdf:pdf},
pages = {2944--2952},
publisher = {Curran Associates, Inc.},
title = {{Learning Continuous Control Policies by Stochastic Value Gradients}},
url = {http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients.pdf},
year = {2015}
}
@incollection{Kulkarni2016,
author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and Garnett, R},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
pages = {3675--3683},
publisher = {Curran Associates, Inc.},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf},
year = {2016}
}
@article{Gloye2005,
abstract = {This paper shows how an omnidirectional robot can learn to correct inaccuracies when driving, or even learn to use corrective motor commands when a motor fails, whether partially or completely. Driving inaccuracies are unavoidable, since not all wheels have the same grip on the surface, or not all motors can provide exactly the same power. When a robot starts driving, the real system response differs from the ideal behavior assumed by the control software. Also, malfunctioning motors are a fact of life that we have to take into account. Our approach is to let the control software learn how the robot reacts to instructions sent from the control computer. We use a neural network, or a linear model for learning the robot's response to the commands. The model can be used to predict deviations from the desired path, and take corrective action in advance, thus increasing the driving accuracy of the robot. The model can also be used to monitor the robot and assess if it is performing according to its learned response function. If it is not, the new response function of the malfunctioning robot can be learned and updated. We show, that even if a robot loses power from a motor, the system can re-learn to drive the robot in a straight path, even if the robot is a black-box and we are not aware of how the commands are applied internally.},
author = {Gloye, Alexander and Wiesel, Fabian and Tenchio, Oliver and Simon, Mark},
doi = {10.1524/itit.2005.47.5_2005.250},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gloye et al. - 2005 - Reinforcing the Driving Quality of Soccer Playing Robots by Anticipation (Verbesserung der Fahreigenschaften von.pdf:pdf},
issn = {2196-7032},
journal = {it - Information Technology},
month = {jan},
number = {5},
pages = {250--257},
publisher = {De Gruyter Oldenbourg},
title = {{Reinforcing the Driving Quality of Soccer Playing Robots by Anticipation (Verbesserung der Fahreigenschaften von fu{\ss}ballspielenden Robotern durch Antizipation)}},
url = {http://www.degruyter.com/view/j/itit.2005.47.issue-5/itit.2005.47.5{\_}2005.250/itit.2005.47.5{\_}2005.250.xml},
volume = {47},
year = {2005}
}
@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.},
author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
pages = {469--483},
title = {{A survey of robot learning from demonstration}},
url = {www.elsevier.com/locate/robot},
volume = {57},
year = {2009}
}
