
@article{nair_massively_2015,
	title = {Massively Parallel Methods for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1507.04296},
	abstract = {We present the Ô¨Årst massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm ({DQN}) (Mnih et al., 2013). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed {DQN} in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
	journaltitle = {{arXiv}:1507.04296 [cs]},
	author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
	urldate = {2018-11-17},
	date = {2015-07-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1507.04296},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Nair et al. - 2015 - Massively Parallel Methods for Deep Reinforcement .pdf:/home/queensgambit/Zotero/storage/LTZGKVBI/Nair et al. - 2015 - Massively Parallel Methods for Deep Reinforcement .pdf:application/pdf}
}

@article{moore_prioritized_nodate,
	title = {Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time},
	abstract = {We present a new algorithm, Prioritized Sweeping, for e cient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Di erencing and Qlearning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of di erent stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have di culty.},
	pages = {40},
	author = {Moore, Andrew W and Atkeson, Christopher G},
	langid = {english},
	file = {Moore and Atkeson - Prioritized Sweeping Reinforcement Learning with .pdf:/home/queensgambit/Zotero/storage/Q3TGHHQN/Moore and Atkeson - Prioritized Sweeping Reinforcement Learning with .pdf:application/pdf}
}