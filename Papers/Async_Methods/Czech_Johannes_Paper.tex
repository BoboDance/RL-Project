%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
    %!PS-Adobe-3.0 EPSF-3.0
    %%BoundingBox: 19 19 221 221
    %%CreationDate: Mon Sep 29 1997
    %%Creator: programmed by hand (JK)
    %%EndComments
    gsave
    newpath
    20 20 moveto
    20 220 lineto
    220 220 lineto
    220 20 lineto
    closepath
    2 setlinewidth
    gsave
    .4 setgray fill
    grestore
    stroke
    grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

    \title{Survey - Asynchronous Methods for Reinforcement Learning%\thanks{Grants or other notes
    %about the article that should go on the front page should be
    %placed here. General acknowledgments should be placed at the end of the article.}
    }
    %\subtitle{Do you have a subtitle?\\ If so, write it here}

    %\titlerunning{Short form of title}        % if too long for running head

    \author{Johannes Czech
    	%         \and
    %Second Author %etc.
    }

    %\authorrunning{Short form of author list} % if too long for running head

    \institute{F. Author \at
    first address \\
    Tel.: +123-45-678910\\
    Fax: +123-45-678910\\
    \email{fauthor@example.com}           %  \\
    %             \emph{Present address:} of F. Author  %  if needed
    \and
    S. Author \at
    second address
    }

    \date{Received: date / Accepted: date}
    % The correct dates will be entered by the editor


    \maketitle

    \begin{abstract}
        %Insert your abstract here. Include keywords, PACS and mathematical
        %subject classification numbers as needed.
        
        This paper aims to address the issue of often high computational requirements for Reinforcement Learning application by validating techniques introduced by asynchronous methods.
        First go over the general principle and the motivation behind asynchronous methods. Brief history when asynchronous where used for RL the first time. Next cover the theory of techniques of shared weight updates.Taking technical challenges into account such as process communication and memory requirements.  Description and discussion of the superlinearity phenomena when using asynchronous methods. Discuss application areas for asynchronous methods and advantages and potential problems when using them.
        
        \keywords{Reinforcement Learning \and Asynchronism \and Super-linearity}
        %\keywords{First keyword \and Second keyword \and More}
        % \PACS{PACS code1 \and PACS code2 \and more}
        % \subclass{MSC code1 \and MSC code2 \and more}
    \end{abstract}


    \section{Introduction}
    \label{intro}
    From a historical perspective asynchronous have appeared very early in the research. One of the earliest use cases of asynchronous methods for RL was in the context of Dynamic Programming (Bertsekas and Tsitsiklis 1989 -> reference).
    Later this technique was built upon in an algorithm called Prioritized Sweeping  -> reference which is acts the same way as the Gauss Seidel algorithm for Dynamic Programming but concentrates all computational effort on the most "interesting" parts of the system.
    \textit{-> shortly go over Dynamic Programming (Bellman 1957)}
    -> \textit{Bellman's optimality equation}
    
    An important result from the theory of Markov decision tasks tells us that there always exists at least one policy which is \textit{optimal} in the following  sense. For every state, the expected discounted reward-to-go using an optimal policy is no worse than that from any other policy" (S.14, "Prioritized Sweeping").
    Makes use of heuristic, introducing a hyperparameter 
    -> Question: What is the main application of asynchronous methods here?
    
    
    Moreover Dimitri P. Bertsekas extended his ideas in a "Distributed Asynchronous Policy Iteration in Dynamic Programming" 2010. -> reference,
   "Asynchronous Methods for Deep Reinforcement Learning" 2016 -> reference
    
    
    
    Your text comes here. Separate text sections with
    Motivation
    History
    Theory
    Mathematical Formulation
    
    
    The revival of asynchronous methods has been started by DeepMind
    Massively Parallel Methods for Deep Reinforcement Learning -> reference
    
    \section{Taxononomy of distributed reinforcement learning}
    Taxonomy -> Paper: Communication Efficient Distributed Reinforcement Learning
    
    Approaches Q-Learning, Policy-Gradient Methods, Actor-Critic Methods
    
    2 settings:
    * multi-agent collaborative RL
    -- multiple agents aim to maximize the team-averaged long-term reward via collaboration in a common environment
    * parallel RL
    -- multiple parallel machines are used for solving large-scale MDPs with larger computational power and higher data efficiency
    
    Distributed Methods are applicable in various RL methods
    * Policy Gradient 
    * Q-Learning (distributed Q-Learning)
    * Actor-Critic Approaches (A3C / A2C )
    
    -> use a figure here to describe the concept
    
    Problem formulation: Multi-agent collaborative RL:
    * each agent m observes globabl state s\_t e S shared by all agents and takes action a\_m,t e A\_m 
    local action is determined by local policy
    
    Problem formulation: Parallel reinforcement learning
    * solving large-scale single agent RL task, running in parallel on multiple computation units e.g. workers
    * advantage training time reduction, training stabilization
    * aim to learn common policy for different instance of identical  MDP
    -> losses and initial state is different for each worker
    
    
    \section{Superlinearity phenomena}
    \label{sec:1}
    Text with citations \cite{RefB} and \cite{RefJ}.
    Try to give proof for this phenomena otherwise rely on empirical experiments
     
    \section{Applications}
    \label{sec:2}
    Describe application areas
    Asynchronous learning with multiple agents
    Swarm robots
    
    as required. Don't forget to give each section
    and subsection a unique label (see Sect.~\ref{sec:1}).
    
    Reference: Deep Reinforcement Learning for Robotic Manipulation with
    Asynchronous Off-Policy Updates
    
    * Addresses problem of high sample complexity for reinforcement learning tasks
    * Makes use of off-policy training of deep Q-functions
    * Learning of complex control tasks without prior demonstration, from their knowledge the first use case of this
    * Speed up learning by using asynchronous methods
    * usage of Deep Deterministic Policy Gradient algorithm (DDPG) and Normalized Advantage Function algorithm (NAF) achieve training times suitable for real robotic systems
    * introduction of new asynchronous variant of NAF
    * focuses on model-free	RL
    * "The
    goals of this prior work are fundamentally different from
    ours: while prior asynchronous deep reinforcement learning
    work seeks to reduce overall training time, under the as-
    sumption that simulation time is inexpensive and the training
    is dominated by neural network computations, our work
    instead seeks to minimize the training time when training
    on real physical robots, where experience is expensive and
    computing neural network backward passes is comparatively
    cheap."
    * don't make use of replay buffer but asynchronous updates instead collecting experience across multiple robotic platforms
    * achieved significant speedup in overall training time
    
    
    Collective Robot Reinforcement Learning with Distributed
    Asynchronous Guided Policy Search
    * making use of asynchronous updates to have greater quantity and diversity of experience
    * task visual-based door opening task
    * guided policy search simultaneously trained with a repay buffer of the latest trajectory samples
    
    \paragraph{Paragraph headings} Use paragraph headings as needed.
    \begin{equation}
        a^2+b^2=c^2
    \end{equation}
    
    \section{Conclusion}
Advantages when using async updates
Disadvantage problems, limits for using asynchronous methods
    % For one-column wide figures use
    \begin{figure}
        % Use the relevant command to insert your figure file.
        % For example, with the graphicx package use
        \includegraphics{example.eps}
        % figure caption is below the figure
        \caption{Please write your figure caption here}
        \label{fig:1}
        % Give a unique label
    \end{figure}
    %
    % For two-column wide figures use
    \begin{figure*}
        % Use the relevant command to insert your figure file.
        % For example, with the graphicx package use
        \includegraphics[width=0.75\textwidth]{example.eps}
        % figure caption is below the figure
        \caption{Please write your figure caption here}
        \label{fig:2}
        % Give a unique label
    \end{figure*}
    %
    % For tables use
    \begin{table}
        % table caption is above the table
        \caption{Please write your table caption here}
        \label{tab:1}       % Give a unique label
        % For LaTeX tables use
        \begin{tabular}{lll}
            \hline\noalign{\smallskip}
            first & second & third  \\
            \noalign{\smallskip}\hline\noalign{\smallskip}
            number & number & number \\
            number & number & number \\
            \noalign{\smallskip}\hline
        \end{tabular}
    \end{table}


    %\begin{acknowledgements}
    %If you'd like to thank anyone, place your comments here
    %and remove the percent signs.
    %\end{acknowledgements}

    % BibTeX users please use one of
    %\bibliographystyle{spbasic}      % basic style, author-year citations
    %\bibliographystyle{spmpsci}      % mathematics and physical sciences
    %\bibliographystyle{spphys}       % APS-like style for physics
    %\bibliography{}   % name your BibTeX data base


%Asynchronous Methods for Deep Reinforcement Learning, https://arxiv.org/pdf/1602.01783.pdf
%Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates, https://ieeexplore.ieee.org/document/7989385
%Massively Parallel Methods for Deep Reinforcement Learning, https://arxiv.org/pdf/1507.04296.pdf
%Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search, https://arxiv.org/pdf/1610.00673.pdf

%
    % Non-BibTeX users please use
    \begin{thebibliography}{}
        %
        % and use \bibitem to create references. Consult the Instructions
        % for authors for reference list style.
        %
        \bibitem{RefJ}
        % Format for Journal Reference
        Author, Article title, Journal, Volume, page numbers (year)
        % Format for books
        \bibitem{RefB}
        Author, Book title, page numbers. Publisher, place (year)
        % etc
    \end{thebibliography}

\end{document}
% end of file template.tex

